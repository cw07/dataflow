# Output Configuration
# Each output has a unique ID that can be referenced in time_series.csv destination field

outputs:
  # Database Outputs
  postgres1:
    id: postgres1
    type: database
    enabled: true
    description: Main PostgreSQL database for production data
    config:
      engine: postgresql
      host: ${POSTGRES_HOST:-localhost}
      port: ${POSTGRES_PORT:-5432}
      database: ${POSTGRES_DB:-dataflow_prod}
      username: ${POSTGRES_USER:-dataflow_user}
      password: ${POSTGRES_PASSWORD}
      schema: public
      connection_pool:
        min_size: 5
        max_size: 20
        timeout: 30
        recycle: 3600  # Recycle connections after 1 hour
      options:
        sslmode: prefer
        connect_timeout: 10
        application_name: dataflow
    write_options:
      batch_size: 1000
      flush_interval: 5  # seconds
      retry_on_error: true
      max_retries: 3
      table_prefix: ts_  # Prefix for dynamically created tables
      create_tables: true
      use_partitioning: true  # Create partitioned tables by month
      partition_column: timestamp

  sqlserver1:
    id: sqlserver1
    type: database
    enabled: true
    description: Main SQL Server database for production data
    config:
      engine: mssql
      host: ${MSSQL_HOST:-10.0.1.50}
      port: ${MSSQL_PORT:-1433}
      database: ${MSSQL_DB:-DataflowAnalytics}
      username: ${MSSQL_USER:-sa}
      password: ${MSSQL_PASSWORD}
      schema: dbo
      driver: ODBC Driver 17 for SQL Server
      connection_pool:
        min_size: 3
        max_size: 15
        timeout: 30
        recycle: 7200
      options:
        encrypt: yes
        trust_server_certificate: no
        connection_timeout: 30
        multi_subnet_failover: yes
    write_options:
      batch_size: 5000
      flush_interval: 10
      retry_on_error: true
      max_retries: 3
      table_prefix: mkt_
      create_tables: true
      use_bulk_insert: true  # Use SQL Server bulk insert for performance

  # Redis Output
  redis1:
    id: redis1
    type: redis
    enabled: true
    description: Redis cache for real-time data distribution
    config:
      host: ${REDIS_HOST:-localhost}
      port: ${REDIS_PORT:-6379}
      database: ${REDIS_DB:-0}
      password: ${REDIS_PASSWORD:-}
      sentinel:  # Optional Redis Sentinel configuration
        enabled: false
        master_name: mymaster
        sentinels:
          - host: sentinel1.example.com
            port: 26379
          - host: sentinel2.example.com
            port: 26379
      cluster:  # Optional Redis Cluster configuration
        enabled: false
        startup_nodes:
          - host: cluster1.example.com
            port: 7000
          - host: cluster2.example.com
            port: 7001
      connection_pool:
        max_connections: 50
        socket_timeout: 5
        socket_connect_timeout: 5
        socket_keepalive: true
        socket_keepalive_options: {}
    write_options:
      patterns:
        - stream  # Use Redis Streams (XADD)
        - pubsub  # Use Pub/Sub for notifications
        - hash    # Store latest values in hash
        - timeseries  # Use RedisTimeSeries module if available
      stream:
        key_prefix: "dataflow:stream:"
        max_length: 10000  # Maximum entries per stream
        approximate: true  # Use ~ for MAXLEN
      pubsub:
        channel_prefix: "dataflow:channel:"
        include_metadata: true
      hash:
        key_prefix: "dataflow:latest:"
        expire_seconds: 3600  # TTL for latest values
      timeseries:
        key_prefix: "dataflow:ts:"
        retention: 86400000  # 1 day in milliseconds
        chunk_size: 4096
        duplicate_policy: LAST  # BLOCK, FIRST, LAST, MIN, MAX, SUM

  # File Outputs
  file1:
    id: file1
    type: file
    enabled: true
    description: Parquet files for long-term data archival
    config:
      format: parquet
      base_path: /data/dataflow/archive
      create_directories: true
      permissions: "0755"
    write_options:
      compression: snappy  # none, snappy, gzip, lz4, zstd
      row_group_size: 100000
      use_dictionary: true
      use_deprecated_int96_timestamps: false
      coerce_timestamps: ms  # ms, us
      partition_by:
        - year
        - month
        - day
        - symbol
      file_naming:
        pattern: "{symbol}_{date}_{time}_{sequence}.parquet"
        date_format: "%Y%m%d"
        time_format: "%H%M%S"
      buffer:
        size_mb: 100  # Flush when buffer reaches 100MB
        timeout_seconds: 300  # Flush every 5 minutes
        records: 100000  # Flush every 100k records

  file2:
    id: file2
    type: file
    enabled: true
    description: CSV files for real-time data export and external system integration
    config:
      format: csv
      base_path: /data/dataflow/realtime
      create_directories: true
      permissions: "0644"
    write_options:
      delimiter: ","
      quoting: minimal  # all, minimal, nonnumeric, none
      encoding: utf-8
      include_header: true
      line_terminator: "\n"
      partition_by:
        - date
        - exchange
        - symbol
      file_naming:
        pattern: "{exchange}_{symbol}_{date}_{sequence}.csv"
        date_format: "%Y%m%d"
      buffer:
        size_mb: 50
        timeout_seconds: 60
        records: 50000
      rotation:
        enabled: true
        strategy: size  # size, time, records
        max_size_mb: 1024  # Rotate after 1GB
        max_age_minutes: 60  # Rotate every hour
        max_records: 1000000  # Rotate after 1M records
        compress_on_rotation: true
        compression_format: gzip